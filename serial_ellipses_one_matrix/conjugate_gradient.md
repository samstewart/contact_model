Introduce problem

Introduce Solution
Move in $A$-orthogonal directions which improves convergence to $n$.

Q: how does this reduce the problem to a circle?
Q: are we iteratively building the transformation?
Q: why should the gram-schmidt matrix be lower triangular? 

Q: why in residual directions?
1. iterative linearly independent set we can convert to an $A$ orthogonal set
2. the definition of the residual means that the transformation mapping $r_i$ to $d_i$ ($A$ orthogonal directions) is itself an orthogonal transformation (preserves angles). 

For arbitrary vector $w$, we can write
\[
	w = \beta v_1 + \alpha v_2
\] 
where $v_1$ is $A$ orthogonal to $v_2$. Then 
\[
\innerp{w}{v_2}\_A = 0 + \alpha \innerp{v_2}{v_2}\_A
\]
so 
\[
\alpha = \frac{w^T A v_2}{v_2 A v_2}
\]


Q: is the inverse of an upper triangular matrix also upper triangular?
Q: what does the Kyrlov subspace property get us?
If $v_i$ are orthogonal in Euclidean space and belong to a Kyrlov subspace generated by $A$, then the Gram-Schmidt algorithm can produce the $A$ orthogonal $w_i$ via a diagonal matrix multiplication instead of a upper triangular matrix. It requires keeping only the top vector in memory, instead of the entire subvectors 

Q: how does Gram-Schmidt work in the case when we have non-euclidean dot product?

Technique:


